<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Subtitle Segmentation</title>
    <link rel="stylesheet" href="main.css">
</head>
<body>
    <header class="site-header">
        <div class="wrapper">
            <h1 class="site-title">Multilingual Subtitle Dataset with Coarse-to-Fine Text Segmentation</h1>
        </div>
    </header>

    <main class="page-content">
        <section>
            <h2>Abstract</h2>
            <p>Accurate text segmentation is essential for text-related generative tasks, including text inpainting, editing, and style transfer. However, current methods often fall short in addressing the complexities of multilingual subtitles, hindered by the lack of a customizable and diverse dataset as well as an efficient pipeline for handling subtitle-specific challenges. In this work, we present ADET80k, a customizable synthetic dataset built upon ADE20k, featuring multilingual subtitle text in four languages—English, Chinese, Spanish, and Korean—with diverse typographical effects. To address subtitle segmentation, we propose a two-stage framework: DeepLabv3+ generates coarse masks, which are subsequently refined using "High Quality Segmentation for Ultra High-resolution Images," achieving precise, high-resolution outputs. Extensive experiments demonstrate the robustness of our framework in multilingual contexts, with applications in subtitle-specific text inpainting and editing tasks. The dataset, pipeline, and code are publicly available at <a href="https://github.com/BillRenCN/Video-inpaiting.git">GitHub</a>.</p>
        </section>
        <section>
            <h2>1. Introduction</h2>
            <p>Text segmentation is the process of isolating text strokes from complex backgrounds in an image, classifying each pixel as part of the text foreground or the background. This task is a cornerstone for numerous applications, including generating high-quality text images, transferring text styles, and removing text from visual scenes. Precise segmentation enables these tasks to deliver practical and visually coherent results. Among these applications, subtitle inpainting in videos presents unique challenges due to the complexity and variability of subtitles. Subtitles often exhibit diverse effects, such as blurring, bounding boxes, and fade-ins, and they commonly appear in multiple languages. Training individual models for each language is not only computationally expensive but also impractical due to the lack of datasets specifically designed for subtitle-related tasks. To address these challenges, we introduce ADET80k, a customizable and scalable dataset for multilingual subtitle segmentation. Built upon the ADE20k dataset, ADET80k incorporates subtitle overlays in four languages—English, Chinese, Japanese, and Korean—alongside four common effects, providing diverse and realistic training scenarios. The dataset’s design allows for seamless expansion to include additional languages or effects, making it highly adaptable for varied subtitle inpainting tasks. We further propose a robust two-phase segmentation framework that leverages this dataset to achieve precise subtitle inpainting. In the first phase, a coarse mask is generated using traditional segmentation models such as DeepLabv3+. The second phase refines this mask using ultra-high-resolution segmentation techniques, resulting in accurate and detailed segmentation maps. This hierarchical approach not only excels in subtitle-specific tasks but also demonstrates strong generalization capabilities for other text inpainting challenges. While text segmentation has been extensively explored, existing approaches predominantly focus on scene text or artistic text. Scene text segmentation methods, often relying on text detection modules, struggle with subtitle-specific challenges due to their limited adaptability. Artistic text segmentation methods, such as WASNet, address the complexities of local stroke shapes and global text structures but are not optimized for multilingual subtitles or their visual effects. By building on these insights, our method bridges this gap, offering a tailored solution for subtitle inpainting with generalizable applications to broader text-related tasks. Our contributions are summarized as follows:</p>
            <ul>
                <li>A large-scale, multilingual dataset, ADET80k, dedicated to subtitle segmentation, with adjustable corpora and diverse visual effects.</li>
                <li>A two-phase segmentation framework that combines coarse-to-fine processing to achieve refined and accurate segmentation masks.</li>
                <li>Demonstration of the versatility of our approach, which not only addresses subtitle inpainting challenges but also extends to general text segmentation and inpainting tasks.</li>
            </ul>
        </section>
        <section>
            <h2>2. Related Work</h2>
            <h3>2.1. Text Segmentation Methods</h3>
            <p>Traditional text segmentation methods relied on thresholding techniques, low-level features, or probabilistic models like Markov Random Fields (MRF) to isolate text from backgrounds. These early approaches achieved limited success in complex scenes. Modern methods leveraging convolutional neural networks (CNNs) and attention mechanisms have shown significant improvement. For example, SMANet employs multi-scale attention modules within an encoder-decoder architecture, while PGTSNet enhances segmentation accuracy by using pre-trained detectors to ground text regions. Additionally, TexRNet integrates character recognition and attention-based similarity to improve segmentation performance. Despite advancements, these methods focus predominantly on static scene text and artistic text, with limited applicability to dynamic subtitles in videos. Subtitles introduce challenges such as temporal consistency and diverse effects (e.g., blurring, fade-ins, and bounding boxes). High-resolution segmentation techniques like DeepLab and latent diffusion models offer promising directions but remain underexplored for subtitle-specific tasks.</p>
            <h3>2.2. Text Segmentation Datasets</h3>
            <p>Robust datasets are pivotal for advancing text segmentation. Early datasets such as ICDAR13 and Total-Text provided pixel-level annotations but were limited in size, with only 462 and 1,555 images respectively. Larger datasets like COCO-TS and BTS addressed scale limitations but often suffered from noisy annotations due to automated labeling. TextSeg introduced higher-quality character-level and word-level annotations, while BTS extended this effort to bilingual text. However, these datasets lack focus on subtitles, which present unique challenges like multilinguality and varying styles. Our proposed dataset, ADET80k, addresses this gap by providing a scalable and customizable resource specifically for subtitle segmentation in both images and videos.</p>
            <h3>2.3. Multilingual Text Segmentation</h3>
            <p>Handling multilingual text segmentation involves challenges stemming from diverse scripts and structures across languages. While datasets like BTS have pioneered efforts in multilingual segmentation, their scale and annotation diversity are limited. Recent transformer-based approaches, such as SegFormer, have demonstrated strong potential for general segmentation tasks but have not been explicitly optimized for subtitle inpainting. Our work bridges this gap by introducing a two-phase coarse-to-fine segmentation framework, tailored to multilingual subtitles in video and image settings.</p>
        </section>
        <section>
            <h2>3. Dataset</h2>
            <h3>3.1. ADET80k Overview</h3>
            <p>To address the lack of multilingual and customizable datasets for subtitle segmentation, we introduce ADET80k, a large-scale dataset designed for training and evaluating subtitle segmentation models. Built upon the ADE20k dataset, ADET80k overlays subtitles in four languages (English, Chinese, Japanese, and Korean) using realistic text effects to simulate subtitles commonly found in videos and images. Each image is accompanied by its corresponding binary mask, which serves as ground truth for segmentation tasks.</p>
            <h3>3.2. Corpus and Sampling</h3>
            <p>The corpus for the subtitles consists of frequently used words and phrases in the respective languages. For English and Korean, subtitles are sampled as word sequences, while for Chinese and Japanese, subtitles are generated as character sequences. The sampling ensures that subtitles remain concise, adhering to natural linguistic constraints. For example, Korean subtitles are restricted to shorter phrases, avoiding excessive lengths that hinder readability.</p>
            <h3>3.3. Subtitle Effects and Annotation</h3>
            <p>To emulate the diversity of real-world subtitles, ADET80k incorporates four types of visual effects:</p>
            <ul>
                <li><strong>Plain White Text:</strong> Simple white text directly overlaid on the image.</li>
                <li><strong>Text with Transparent Box:</strong> Subtitles are displayed over a semi-transparent black box for enhanced readability.</li>
                <li><strong>Shadowed Text:</strong> Subtitles include a shadow effect, creating a 3D-like appearance.</li>
                <li><strong>None:</strong> A baseline effect where the text is directly added to the image without additional styling.</li>
            </ul>
            <p>Each effect is randomly applied during the dataset generation process, ensuring visual variability across samples. Figure placeholders are provided for illustrative purposes.</p>
            <div class="img-grid">
                <figure>
                    <img src="img/en_img.jpg" alt="English Subtitle Example">
                    <figcaption>English Subtitle</figcaption>
                </figure>
                <figure>
                    <img src="img/ch_img.jpg" alt="Chinese Subtitle Example">
                    <figcaption>Chinese Subtitle</figcaption>
                </figure>
                <figure>
                    <img src="img/ja_img.jpg" alt="Japanese Subtitle Example">
                    <figcaption>Japanese Subtitle</figcaption>
                </figure>
                <figure>
                    <img src="img/ko_img.jpg" alt="Korean Subtitle Example">
                    <figcaption>Korean Subtitle</figcaption>
                </figure>
            </div>

            <h3>3.4. Dataset Composition</h3>
            <p>The ADET80k dataset contains 80,000 samples, with each image in the ADE20k dataset processed to generate four variations, one for each language. For each image:</p>
            <ul>
                <li>Subtitles in all four languages are generated and rendered.</li>
                <li>Corresponding binary masks are created, where subtitle regions are marked as white (255) and the background as black (0).</li>
            </ul>
            <h3>3.5. Dataset Generation Pipeline</h3>
            <p>The dataset generation process is implemented using the Python Imaging Library (PIL). The pipeline includes:</p>
            <ol>
                <li><strong>Text Sampling:</strong> Text units are selected randomly from the corpus, ensuring subtitles fit within the image width. For each language, appropriate linguistic constraints are applied, such as limiting Korean subtitles to six units.</li>
                <li><strong>Subtitle Rendering:</strong> Subtitles are rendered with one of the predefined effects (plain, box, shadow, or none) using PIL’s text and drawing utilities.</li>
                <li><strong>Binary Mask Creation:</strong> A binary mask corresponding to the subtitle's position is generated, serving as ground truth for segmentation tasks.</li>
                <li><strong>Multilingual Variations:</strong> For each image, subtitles in four languages are created, ensuring language diversity across the dataset.</li>
            </ol>
        </section>
        <section>
            <h2>4. Subtitle Segmentation</h2>
            <h3>4.1. Pipeline Overview</h3>
            <p>The segmentation pipeline consists of two main stages:</p>
            <ol>
                <li><strong>Coarse Mask Generation:</strong> Given an input image, DeepLabv3+ is trained on the ADET80k dataset to produce an initial coarse mask of subtitle regions. This stage identifies the general subtitle area but may lack precise boundaries and finer details.</li>
                <li><strong>Mask Refinement:</strong> The coarse mask is further refined using CRM, a high-quality segmentation model also trained on the ADET80k dataset. CRM employs super-resolution techniques to progressively improve the mask, ensuring precise subtitle boundaries and enhanced fidelity.</li>
            </ol>
            <p>This two-stage process strikes a balance between computational efficiency and segmentation precision, enabling robust performance across diverse languages and subtitle styles.</p>
            <h3>4.2. Evaluation on YouTube Subtitles</h3>
            <p>To evaluate the effectiveness of our pipeline, we tested it on subtitles extracted from YouTube videos in English and Japanese. Figure placeholders are provided for illustrative purposes.</p>
            <div class="img-grid">
                <figure>
                    <img src="img/you_en_img.jpg" alt="YouTube English Subtitle">
                    <figcaption>YouTube English Subtitle</figcaption>
                </figure>
                <figure>
                    <img src="img/you_en_seg.png" alt="Segmented English Subtitle">
                    <figcaption>Segmented English Subtitle</figcaption>
                </figure>
                <figure>
                    <img src="img/you_ja_img.png" alt="YouTube Japanese Subtitle">
                    <figcaption>YouTube Japanese Subtitle</figcaption>
                </figure>
                <figure>
                    <img src="img/you_ja_seg.png" alt="Segmented Japanese Subtitle">
                    <figcaption>Segmented Japanese Subtitle</figcaption>
                </figure>
            </div>
        </section>
        </section>
        <section>
            <h2>References</h2>
            <p>[1] Silvia Bonechi, Paolo Andreini, Monica Bianchini, and Fabio Scarselli. Coco ts dataset: Pixel-level annotations based on weak supervision for scene text segmentation. In International Conference on Artificial Neural Networks, pages 238–250, 2019.</p>
            <p>[2] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 801–818, 2018.</p>
            <p>[3] Chee Kheng Ch’ng and Chee Seng Chan. Total-text: A comprehensive dataset for scene text detection and recognition. In Proceedings of the International Conference on Document Analysis and Recognition, pages 935–942, 2017.</p>
            <p>[4] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, and Lluis Gomez-Bigorda Mestre. Icdar 2013 robust reading competition. In Proceedings of the International Conference on Document Analysis and Recognition, pages 1484–1493, 2013.</p>
            <p>[5] Anand Mishra, Karteek Alahari, and C.V. Jawahar. An mrf model for binarization of natural scene text. In Proceedings of the 2011 International Conference on Document Analysis and Recognition, pages 11–16, 2011.</p>
            <p>[6] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674–10685, 2022.</p>
            <p>[7] Bing Su, Shijian Lu, and Chew Lim Tan. Binarization of historical document images using the local maximum and minimum. In Proceedings of the 9th IAPR International Workshop on Document Analysis Systems, pages 159–166, 2010.</p>
            <p>[8] Xiangrong Wang, Lei Huang, and Chenglin Liu. A novel method for embedded text segmentation based on stroke and color. In Proceedings of the 2011 International Conference on Document Analysis and Recognition, pages 151–155, 2011.</p>
            <p>[9] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In Advances in Neural Information Processing Systems, volume 34, pages 12077–12090, 2021.</p>
            <p>[10] Xingqian Xu, Zengyuan Qi, Jian Ma, Haichao Zhang, Yuming Shan, and Xuefeng Qie. Bts: A bi-lingual benchmark for text segmentation in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19152–19162, 2022.</p>
            <p>[11] Xingqian Xu, Zhiwei Zhang, Zhaowen Wang, Brian Price, Zhifei Wang, and Humphrey Shi. Rethinking text segmentation: A novel dataset and a text-specific refinement approach. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12045–12055, 2021.</p>
        </section>


        <section>
            <h2>Citation</h2>
            <p>If you find our work useful, please cite:</p>
            <pre><code>@misc{ren2024multilingual,
        title={Multilingual Subtitle Dataset with Coarse-to-Fine Text Segmentation},
        url={https://billrencn.github.io/Multilingual-Text-Seg/},
        author={Ren, Jie and Lee, Yong Jae},
        year={2024}
    }</code></pre>
        </section>

    </main>
    <footer class="site-footer">
        <div class="wrapper">
            <p>© 2025. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>

